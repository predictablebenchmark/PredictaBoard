{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d87f5c27dafdc9e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T22:13:20.945181Z",
     "start_time": "2024-12-10T22:13:20.895937Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d87f5c27dafdc9e2",
    "outputId": "b528b040-72b4-487e-f2e6-09ae5fe9c3cb"
   },
   "outputs": [],
   "source": [
    "# enable reloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T22:13:24.000794Z",
     "start_time": "2024-12-10T22:13:20.964502Z"
    },
    "collapsed": true,
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from embeddings_utils.utils import load_with_conditions\n",
    "from embeddings_utils.classification_utils import brierDecomp, arc_points, compute_winklers_score\n",
    "from generate_train_test_split import load_open_llm_v2\n",
    "\n",
    "from typing import List, Tuple, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "577ce0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_labels_predictive_method = {\n",
    "    'logistic_regression_l2': 'Logistic Regression (l2)',\n",
    "    'logistic_regression_l1_c=1': 'Logistic Regression (l1)',\n",
    "    'logistic_regression_l1_c=0.1': 'Logistic Regression (l1, c=0.1)',\n",
    "    'xgboost': 'XGBoost',\n",
    "}\n",
    "\n",
    "short_labels_features = {\n",
    "    'openai': 'OpenAI',\n",
    "    'word2vec': 'Word2Vec',\n",
    "    'fasttext': 'FastText',\n",
    "    'ngrams_1': 'Ngrams-1',\n",
    "    'fineTunedLlama': 'FT-Llama'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea5b1ce",
   "metadata": {},
   "source": [
    "# Run configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51d4a8ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T22:13:26.610499Z",
     "start_time": "2024-12-10T22:13:24.420378Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = os.path.join(\"results\", \"mmlu_pro_assessor_results.pkl\")\n",
    "\n",
    "# Learn assessors for all LLMs in the MMLU Pro dataset\n",
    "mmlu_pro_results_file_location = os.path.join(\"..\",\"..\",\"data\", \"open-llm-leaderboard-v2\", \"mmlu_pro_results.csv\")\n",
    "# Some model are duplicated, so use set() to dedupe\n",
    "llms = list(set(pd.read_csv(mmlu_pro_results_file_location)[\"model\"]))\n",
    "train_dataset_name = \"mmlu_pro\"\n",
    "test_dataset_name = \"mmlu_pro\" # In distribution assessment\n",
    "PVR_thresholds = [0.8, 0.9, 0.95]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4368b8e896ee545",
   "metadata": {
    "collapsed": false,
    "id": "4368b8e896ee545"
   },
   "source": [
    "## Load results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "686700cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T22:19:26.413975Z",
     "start_time": "2024-12-10T22:13:27.109723Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load train/test/val split data\n",
    "train_df, validation_df, test_df = load_open_llm_v2(llms, train_dataset_name, test_dataset_name, exclude_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ab8f2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:15:33.512520Z",
     "start_time": "2024-12-10T23:15:28.030325Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load results generated by train_embeddings_assessors.ipynb\n",
    "assessors_results_df = load_with_conditions(filename)\n",
    "# Remove l1_c=0.1 assessors, as the regularization seems to be too much (they normally become a constant assessor)\n",
    "assessors_results_df = assessors_results_df[assessors_results_df[\"predictive_method\"] != \"logistic_regression_l1_c=0.1\"]\n",
    "print(f\"Loaded number considered LLMs: {len(set(assessors_results_df['llm']))}\")\n",
    "print(f\"Loaded number of LLM-assessor pairs: {len(assessors_results_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8384497d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:16:14.706441Z",
     "start_time": "2024-12-10T23:16:12.820677Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load results of using a fine tuned Llama 1B to predict\n",
    "# Excluded for now as Since we're trying to provide a benchmark, and LLama was getting Winkler's scores around -2.2 (while the others were getting ~+/- 0.3)\n",
    "# LLAMA_PREDICTIVE_METHOD = LLAMA_FEATURES = \"FT-Llama\"\n",
    "\n",
    "# llama_assessor_results_loc = os.path.join(\"..\", \"..\", \"results\")\n",
    "# files = [item for item in os.listdir(llama_assessor_results_loc) if item[-13:] == \"_mmlu_pro.csv\"]\n",
    "# llms_assessed = [file[:-35] for file in files]\n",
    "# for file in files:\n",
    "#     # Add a row to the results df for the assessed LLM\n",
    "#     with open(os.path.join(llama_assessor_results_loc, file)) as csvfile:\n",
    "#         fine_tuned_llama_results_csv = csv.reader(csvfile)\n",
    "#         fine_tuned_llama_results_df = pd.DataFrame(fine_tuned_llama_results_csv, columns=next(fine_tuned_llama_results_csv))\n",
    "#         # Double check that the results df only contains the test items\n",
    "#         assert set(test_df[\"question_id\"]) == set(fine_tuned_llama_results_df[\"id\"])\n",
    "#         y_pred = fine_tuned_llama_results_df[\"assessor_correct\"].astype(float)\n",
    "#         labels = fine_tuned_llama_results_df[\"llm_correct\"].astype(float)\n",
    "#         llm_accuracy_test = labels.mean()\n",
    "#         # # TODO: Assuming assessor_correct is the proportion of times the assessor predicts that the llm will be correct\n",
    "#         BrierScore, Calibration, Refinement = brierDecomp(\n",
    "#             y_pred, labels\n",
    "#         )\n",
    "#         win_score = compute_winklers_score(labels, y_pred)\n",
    "#         roc_auc = roc_auc_score(labels, y_pred)\n",
    "#         arc = arc_points(labels, y_pred)\n",
    "#         llm_row = pd.DataFrame({'predictive_method': LLAMA_PREDICTIVE_METHOD, 'features': LLAMA_FEATURES, 'llm': file[:-35], 'BrierScore_val': np.nan,\n",
    "#         'Calibration_val': np.nan, 'Refinement_val': np.nan, 'AUROC_val': np.nan, 'WinklerScore_val': np.nan, 'BrierScore_test': BrierScore,\n",
    "#         'Calibration_test': Calibration, 'Refinement_test': Refinement, 'AUROC_test': roc_auc, 'WinklerScore_test': win_score,\n",
    "#         'predictions_train': None, 'predictions_val': None, 'predictions_test': None, 'arc_train': None,\n",
    "#         'arc_test': [arc], 'arc_val': None, 'trained_classifier': None, 'llm_accuracy_train': np.nan, 'llm_accuracy_val': np.nan, 'llm_accuracy_test': llm_accuracy_test,})\n",
    "#         assessors_results_df = pd.concat([assessors_results_df, llm_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47513ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the names with abbreviated versions\n",
    "assessors_results_df[\"predictive_method\"] = assessors_results_df[\"predictive_method\"].replace(short_labels_predictive_method)\n",
    "assessors_results_df[\"features\"] = assessors_results_df[\"features\"].replace(short_labels_features)\n",
    "\n",
    "# now sort them so that OAI, W2V, FT, NG1 are together\n",
    "# Define a custom order for embeddings\n",
    "feature_order = ['openai', 'word2vec', 'fasttext', 'ngrams_1']\n",
    "\n",
    "# Convert to a categorical type with the desired order\n",
    "assessors_results_df[\"feature_order\"] = pd.Categorical(assessors_results_df[\"features\"], categories=feature_order, ordered=True)\n",
    "\n",
    "# Now sort by feature_order, and then by predictive method\n",
    "assessors_results_df = assessors_results_df.sort_values([\"feature_order\", \"predictive_method\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63821d18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:16:16.097974Z",
     "start_time": "2024-12-10T23:16:16.006375Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add PVR for each threshold\n",
    "def get_PVR_for_threshold(threshold: float) -> Callable[[List[Tuple[float,float]]], float]:\n",
    "    return lambda vals: 1 - min([rate for rate,acc in vals if acc >= threshold])\n",
    "for threshold in PVR_thresholds:\n",
    "    PVR_col_name = f\"{threshold} PVR\"\n",
    "    assessors_results_df[PVR_col_name] = assessors_results_df[\"arc_test\"].apply(get_PVR_for_threshold(threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdbb428d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:16:16.794607Z",
     "start_time": "2024-12-10T23:16:16.731539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add an llm_method_features name for each row\n",
    "assessors_results_df[\"pair_name\"] = assessors_results_df.apply(lambda row: f\"{row['llm'].replace('__', '/')}\\n({row['predictive_method']}, {row['features']})\", axis=1)\n",
    "# Add a method_features name for each row\n",
    "assessors_results_df[\"predictive_method_features\"] = assessors_results_df.apply(lambda row: f\"{row['predictive_method']}_{row['features']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4081b6f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:16:22.056609Z",
     "start_time": "2024-12-10T23:16:22.027469Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total number considered LLMs: {len(set(assessors_results_df['llm']))}\")\n",
    "print(f\"Total number of LLM-assessor pairs: {len(assessors_results_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae4703",
   "metadata": {},
   "source": [
    "# Comparing the effectiveness of different assessors\n",
    "\n",
    "Firstly we compare the different methods for assessment by grouping by assessor type and making boxplots for AUROC, Brier Score, and 90% PVR. A threshold of 90% was chosen because it is still a significant challenge for assessors but we still have assessors with a relatively good score (A PVR of ~0.2). These plots demonstrate that (1) no one method is significantly better than all others and (2) very few LLM-assessor pairs are able to achieve a PVR substantially above 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e980c35a1c2b74c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:18:25.619419Z",
     "start_time": "2024-12-10T23:18:22.553361Z"
    }
   },
   "outputs": [],
   "source": [
    "side_by_side = False\n",
    "\n",
    "boxplot_params = {\n",
    "    \"showmeans\": True,\n",
    "    \"meanline\": True,\n",
    "    \"boxprops\": dict(linewidth=1),\n",
    "    \"whiskerprops\": dict(linewidth=1),\n",
    "    \"capprops\": dict(linewidth=1),\n",
    "    \"flierprops\": dict(marker='o', markersize=5),\n",
    "    \"medianprops\": dict(linewidth=1),\n",
    "    \"meanprops\": dict(linewidth=1),\n",
    "    \"width\": 0.5\n",
    "}\n",
    "\n",
    "if not side_by_side:\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(6, 12), sharex=True)\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Left panel: AUROC\n",
    "sns.boxplot(data=assessors_results_df, x='predictive_method', hue=\"features\", y='AUROC_test', ax=axes[0], **boxplot_params)\n",
    "axes[0].legend_.remove()  # Remove the legend from the first subplot since it gets in the way and we can see it in the second\n",
    "axes[0].set_ylabel('AUROC (→)', fontsize=12)\n",
    "axes[0].set_title('AUROC by Assessor Method', fontsize=14)\n",
    "axes[0].tick_params(axis='x', rotation=10)\n",
    "axes[0].set_xlabel(\"\")\n",
    "axes[0].axhline(y=0.5, color=\"r\")\n",
    "\n",
    "# Right panel: Winkler's Score\n",
    "sns.boxplot(data=assessors_results_df, x='predictive_method', hue=\"features\", y='WinklerScore_test', ax=axes[1], **boxplot_params)\n",
    "axes[1].set_xlabel(\"\")\n",
    "axes[1].set_ylabel(\"Winkler's Score (→)\", fontsize=12)\n",
    "axes[1].set_title(\"Winkler's Score by Assessor Method\", fontsize=14)\n",
    "axes[1].tick_params(axis='x', rotation=10)\n",
    "axes[1].axhline(y=0, color=\"r\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"experiments_AUROC_Winkler.pdf\", format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a76859",
   "metadata": {},
   "source": [
    "# Finding the best LLM-Assessor pairs by PVR\n",
    "\n",
    "The main metric we are interested in for assessors is the size of the PVR, as this is the size of the region in which the LLM can operate safely. Firstly we consider the top subject-assessor pairs for PVR thresholds 0.8, 0.9 and 0.95 (With a 0.99 threshold, it drops to near zero).\n",
    "\n",
    "This is visualised in the below heatmap, showing the union of the top 10 for each threshold.\n",
    "\n",
    "We can see that the LLM-assessor pairs get a fairly good score at a threshold of 0.8. This is to be expected when the LLMs are fairly good at the task, as the assessor can predict success most of the time. When the threshold is raised to 0.9 we see a very large drop in PVR, as now there is a greater requirement for assessors to make predictions that the LLM will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e60297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_TOP_NUM = 5\n",
    "PVR_thresholds = [0.80, 0.90, 0.95]\n",
    "# Identify the top pairs for each threshold\n",
    "top_pairs_sets = []\n",
    "for threshold in PVR_thresholds:\n",
    "    PVR_col_name = f\"{threshold} PVR\"\n",
    "    top_pairs_for_threshold = assessors_results_df.nlargest(PLOT_TOP_NUM, PVR_col_name)['pair_name'].unique()\n",
    "    top_pairs_sets.append(set(top_pairs_for_threshold))\n",
    "# Take the union of top pairs across all thresholds\n",
    "all_top_pairs = set.union(*top_pairs_sets)\n",
    "# Filter the dataframe to only these pairs\n",
    "filtered_df = assessors_results_df[assessors_results_df['pair_name'].isin(all_top_pairs)]\n",
    "# Keep only the relevant columns: pair_name and PVR columns\n",
    "PVR_cols = [f\"{t} PVR\" for t in PVR_thresholds]\n",
    "filtered_df = filtered_df[['pair_name'] + PVR_cols].drop_duplicates('pair_name')\n",
    "# Set pair_name as the index\n",
    "filtered_df = filtered_df.set_index('pair_name')\n",
    "# Round values to 3 decimal places\n",
    "filtered_df = filtered_df.round(3)\n",
    "# Sort by the 0.9 PVR column\n",
    "filtered_df = filtered_df.sort_values(by=\"0.9 PVR\", ascending=False)\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(filtered_df, annot=True, cmap='YlGnBu', fmt='.3f')\n",
    "plt.ylabel(\"LLM-Assessor Pair Name\")\n",
    "plt.xlabel(\"Threshold PVR\")\n",
    "plt.tight_layout()\n",
    "# plt.rcParams.update({'font.size': 14})\n",
    "# plt.savefig(\"experiments_PPR.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f092431",
   "metadata": {},
   "source": [
    "# ARC curve comparison\n",
    "\n",
    "To demonstrate how assessors can vary in usefulness, we select the highest accuracy LLM (OpenAI__GPT-4o-2024-08-06) and compare the ARC curves for the assessors with the highest, and lowest PVR at a threshold of 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd6a3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_arc_curves(pairs_to_plot: List[str], title = None) -> None:\n",
    "    llm_auc = pd.DataFrame(columns=[\"Assessor Name\", \"Rejection Rate\", \"Accuracy\"])\n",
    "\n",
    "    for _, row in assessors_results_df[\n",
    "            [\"pair_name\", \"arc_test\", \"0.9 PVR\"]\n",
    "        ].iterrows():\n",
    "        res = row[\"arc_test\"]\n",
    "        if row[\"pair_name\"] not in pairs_to_plot:\n",
    "            # Don't plot all the graphs\n",
    "            continue\n",
    "\n",
    "        llm_auc = pd.concat(\n",
    "            [\n",
    "                llm_auc,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Assessor Name\": row[\"pair_name\"],\n",
    "                        \"Rejection Rate\": [i for i, _ in res],\n",
    "                        \"Accuracy\": [j for _, j in res],\n",
    "                    }\n",
    "                ),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    # plot using seaborn\n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    g = sns.relplot(\n",
    "        data=llm_auc, kind=\"line\", x=\"Rejection Rate\", y=\"Accuracy\", hue=\"Assessor Name\", facet_kws={'legend_out': False}\n",
    "    )\n",
    "    if title is not None:\n",
    "        g.figure.suptitle(title, y=1.02)\n",
    "    g.set(ylim=(0.5, 1))\n",
    "    plt.legend(\n",
    "        loc='upper center',\n",
    "        bbox_to_anchor=(0.5, .25),\n",
    "        ncol=1\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"experiments_ARC_crossing_pair.pdf\", format=\"pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "659c8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ARC curves for all assessors, ie all predictive method and all features. Use different line colors across features and different line styles across predictive methods\n",
    "def plot_all_arc_curves(llm):\n",
    "    llm_auc = pd.DataFrame(columns=[\"Assessor Name\", \"Rejection Rate\", \"Accuracy\", \"Features\", \"Predictive Method\"])\n",
    "    assessors_results_df_llm = assessors_results_df[assessors_results_df[\"llm\"] == llm]\n",
    "    \n",
    "    for _, row in assessors_results_df_llm[\n",
    "            [\"pair_name\", \"arc_test\", \"features\", \"predictive_method\"]\n",
    "        ].iterrows():\n",
    "        res = row[\"arc_test\"]\n",
    "        llm_auc = pd.concat(\n",
    "            [\n",
    "                llm_auc,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"Assessor Name\": row[\"pair_name\"],\n",
    "                        \"Rejection Rate\": [i for i, _ in res],\n",
    "                        \"Accuracy\": [j for _, j in res],\n",
    "                        \"Features\": row[\"features\"],\n",
    "                        \"Predictive Method\": row[\"predictive_method\"]\n",
    "                    }\n",
    "                ),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.relplot(\n",
    "        data=llm_auc, kind=\"line\", x=\"Rejection Rate\", y=\"Accuracy\", hue=\"Features\", style=\"Predictive Method\", facet_kws={'legend_out': False}\n",
    "    )\n",
    "    plt.legend(\n",
    "        loc='upper center',\n",
    "        bbox_to_anchor=(0.5, .3),\n",
    "        ncol=2\n",
    "    )\n",
    "    plt.ylim(0.65, 1)\n",
    "    plt.savefig(\"experiments_ARC_top_scorer.pdf\", format=\"pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e6559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all ARC curves for the top accuracy LLM\n",
    "all_results = pd.concat([train_df, validation_df, test_df], ignore_index=True)\n",
    "print(f\"Total instances in the dataset: {len(all_results)}\")\n",
    "num_samples = all_results.shape[0]\n",
    "llm_columns = [col for col in all_results.columns if col.startswith('Success_')]\n",
    "\n",
    "accuracy_per_llm = pd.DataFrame({\n",
    "    # Remove \"Success_model_outputs_\" from the column name\n",
    "    \"llm\": [col_name[8:] for col_name in llm_columns],\n",
    "    \"Accuracy\": [all_results[col].sum()/num_samples for col in llm_columns]  # This works if values are boolean (True counts as 1)\n",
    "})\n",
    "top_accuracy_llm = accuracy_per_llm.loc[accuracy_per_llm[\"Accuracy\"].idxmax(), \"llm\"]\n",
    "print(f\"Top accuracy LLM: {top_accuracy_llm}\")\n",
    "\n",
    "plot_all_arc_curves(top_accuracy_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacd493",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arc_curves([\n",
    "    \"OpenAI/GPT-4o-2024-08-06\\n(Logistic Regression (l1), Ngrams-1)\", \n",
    "    \"MaziyarPanahi/calme-2.1-qwen2.5-72b\\n(Logistic Regression (l1), OpenAI)\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f262c08f85a636",
   "metadata": {},
   "source": [
    "# Table/graph with LLM accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b404be26fbedf840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example family mapping logic:\n",
    "def get_family(llm_name):\n",
    "    llm_name_lower = llm_name.lower()\n",
    "    if \"openai__gpt-\" in llm_name_lower:\n",
    "        return \"OpenAI\"\n",
    "    elif \"llama-3\" in llm_name_lower or \"meta-llama\" in llm_name_lower or \"nousresearch__hermes\" in llm_name_lower:\n",
    "        return \"Llama\"\n",
    "    elif \"mistral\" in llm_name_lower:\n",
    "        return \"Mistral\"\n",
    "    elif \"qwen\" in llm_name_lower:\n",
    "        return \"Qwen\"\n",
    "    elif \"gemma\" in llm_name_lower:\n",
    "        return \"Gemma\"\n",
    "    elif \"pythia\" in llm_name_lower:\n",
    "        return \"Pythia\"\n",
    "    elif \"intel__neural-chat\" in llm_name_lower:\n",
    "        return \"Intel Neural Chat\"\n",
    "    # Add more conditions as needed to categorize your models\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Create a family column based on the llm name\n",
    "assessors_results_df['family'] = assessors_results_df['llm'].apply(get_family)\n",
    "\n",
    "# Create a table with the accuracy of all LLMs\n",
    "llm_accuracy_table = assessors_results_df[[\"llm\", \"llm_accuracy_test\", \"family\"]]\\\n",
    "    .drop_duplicates()\\\n",
    "    .sort_values(by=\"llm_accuracy_test\", ascending=False)\n",
    "\n",
    "print(llm_accuracy_table)\n",
    "\n",
    "# Plot a graph with the accuracy of all LLMs, using family for hue\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(data=llm_accuracy_table, x=\"llm\", y=\"llm_accuracy_test\", hue=\"family\", dodge=False)\n",
    "\n",
    "# add the value in the bars\n",
    "# Iterate over each container (one per hue category)\n",
    "for container in ax.containers:\n",
    "    # ax.bar_label(container, fmt='%.3f', label_type='edge')  # Choose a suitable label_type\n",
    "    # make them vertical\n",
    "    ax.bar_label(container, label_type='edge', rotation=90, fmt='%.3f')\n",
    "\n",
    "plt.xlabel('LLM', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Accuracy of all LLMs', fontsize=14)\n",
    "plt.xticks(rotation=90)\n",
    "#plt.legend(title='Family', bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "#plt.tight_layout()\n",
    "plt.savefig(\"llm_accuracy.pdf\", format=\"pdf\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "pytest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
