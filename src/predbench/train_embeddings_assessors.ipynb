{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d87f5c27dafdc9e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:54:01.220187Z",
     "start_time": "2024-07-10T08:54:00.473706Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d87f5c27dafdc9e2",
    "outputId": "b528b040-72b4-487e-f2e6-09ae5fe9c3cb"
   },
   "outputs": [],
   "source": [
    "# enable reloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T08:54:07.705160Z",
     "start_time": "2024-07-10T08:54:01.228578Z"
    },
    "collapsed": true,
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from embeddings_utils.results_loaders import ngram_vectorize_new, select_features\n",
    "from embeddings_utils.utils import load_with_conditions\n",
    "from embeddings_utils.classification_utils import predictive_method_list, evaluate_and_update_arrays, evaluate_and_update\n",
    "from generate_train_test_split import load_open_llm_v2\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea5b1ce",
   "metadata": {},
   "source": [
    "# Run configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_res = True\n",
    "filename = os.path.join(\"results\", \"mmlu_pro_assessor_results.pkl\")\n",
    "\n",
    "# Learn assessors for all LLMs in the MMLU Pro dataset\n",
    "mmlu_pro_results_file_location = os.path.join(\"..\",\"..\",\"data\", \"open-llm-leaderboard-v2\", \"mmlu_pro_results.csv\")\n",
    "# Some model are duplicated, so use set() to dedupe\n",
    "llms = list(set(pd.read_csv(mmlu_pro_results_file_location)[\"model\"]))\n",
    "train_dataset_name = \"mmlu_pro\"\n",
    "test_dataset_name = \"mmlu_pro\" # In distribution assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bffed68",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df, test_df = load_open_llm_v2(llms, train_dataset_name, test_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b4389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessors_results_df = load_with_conditions(filename, overwrite_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6c29fed5aaa54a",
   "metadata": {
    "collapsed": false,
    "id": "bf6c29fed5aaa54a"
   },
   "source": [
    "## Train assessors for each LLM independently\n",
    "\n",
    "- Train an assessor for each LLM on the train_dataset, then evaluate on each of the test datasets\n",
    "- We train assessors using different features (OpenAI embeddings, word2vec, fasttext and n-grams) and different base classifiers (LogisticRegression, XGBoost) in predictive_method_list. The cells below train the assessors and store the instance-level predictions (and other performance metrics) in a dataframe.\n",
    "- TODO: Describe file outputs (e.g. we store the instance-level predictions in a separate file from the one where the raw model output and ground truths are.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b66ef4c9b9d1375",
   "metadata": {
    "id": "2b66ef4c9b9d1375"
   },
   "source": [
    "Split into different cells for OpenAI embeddings, word2vec, fasttext and n-grams (to avoid memory issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaccefba586ce40",
   "metadata": {
    "collapsed": false,
    "id": "caaccefba586ce40"
   },
   "source": [
    "OpenAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ed1d5b6aa54fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T13:44:00.151613Z",
     "start_time": "2024-07-09T13:43:49.588717Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d65ed1d5b6aa54fe",
    "outputId": "ba40f7ea-2bfc-4d7b-cc10-d8e4892c855a"
   },
   "outputs": [],
   "source": [
    "for llm in llms:\n",
    "    if len(train_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "        print(\n",
    "            f\"Skipping {llm} because there is only one value in the 'Success' column for the train df\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    if len(validation_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "        print(\n",
    "            f\"Skipping {llm} because there is only one value in the 'Success' column for the validation df\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    if len(test_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "        print(\n",
    "            f\"Skipping {llm} because there is only one value in the 'Success' column for the test df\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    for predictive_method, kwargs, pred_method_name in predictive_method_list:\n",
    "        assessors_results_df = evaluate_and_update(\n",
    "                assessors_results_df,\n",
    "                train_df,\n",
    "                validation_df,\n",
    "                test_df,\n",
    "                [\"openai_embeddings\"],\n",
    "                predictive_method,\n",
    "                pred_method_name,\n",
    "                \"openai\",\n",
    "                llm,\n",
    "                filename,\n",
    "                **kwargs,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b41ae7cc870be",
   "metadata": {
    "collapsed": false,
    "id": "69b41ae7cc870be"
   },
   "source": [
    "Word2vec + fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be156221",
   "metadata": {},
   "outputs": [],
   "source": [
    "for llm in llms:\n",
    "    if len(train_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "        print(\n",
    "            f\"Skipping {llm} because there is only one value in the 'Success' column for the train df\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    if len(validation_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "        print(\n",
    "            f\"Skipping {llm} because there is only one value in the 'Success' column for the validation df\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    if len(test_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "        print(\n",
    "            f\"Skipping {llm} because there is only one value in the 'Success' column for the test df\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    for embedding_type in [\"word2vec\", \"fasttext\"]:\n",
    "\n",
    "        for predictive_method, kwargs, pred_method_name in predictive_method_list:\n",
    "\n",
    "            assessors_results_df = evaluate_and_update(\n",
    "                assessors_results_df,\n",
    "                train_df,\n",
    "                validation_df,\n",
    "                test_df,\n",
    "                [\n",
    "                    f\"{embedding_type}_embeddings\"\n",
    "                ],  # TODO: Could also include the response options as a feature\n",
    "                predictive_method,\n",
    "                pred_method_name,\n",
    "                embedding_type,\n",
    "                llm,\n",
    "                filename,\n",
    "                **kwargs,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0cb90e1ccca9a1",
   "metadata": {
    "collapsed": false,
    "id": "ef0cb90e1ccca9a1"
   },
   "source": [
    "ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a05d0dc33ba61a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T13:47:23.468600Z",
     "start_time": "2024-07-09T13:47:19.841976Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7a05d0dc33ba61a",
    "outputId": "d634e933-c9b0-4c9d-f5cb-f6ca58180c30"
   },
   "outputs": [],
   "source": [
    "for n_gram_size in [1]:\n",
    "    # compute the n-grams\n",
    "    X_train_ngrams, X_val_ngrams, X_test_ngrams, vectorizer = ngram_vectorize_new(\n",
    "        train_df[\"prompt\"],\n",
    "        validation_df[\"prompt\"],\n",
    "        test_df[\"prompt\"],\n",
    "        ngram_range=(1, n_gram_size),\n",
    "    )\n",
    "\n",
    "    for llm in llms:\n",
    "        if len(train_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(\n",
    "                f\"Skipping {llm} because there is only one value in the 'Success' column for the train df\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        if len(validation_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(\n",
    "                f\"Skipping {llm} because there is only one value in the 'Success' column for the validation df\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        if len(test_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(\n",
    "                f\"Skipping {llm} because there is only one value in the 'Success' column for the test df\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # select the features (this depends on which LLM you are considering)\n",
    "        (\n",
    "            X_train_ngrams_selected,\n",
    "            X_val_ngrams_selected,\n",
    "            X_test_ngrams_selected,\n",
    "            selector,\n",
    "        ) = select_features(\n",
    "            X_train_ngrams, train_df[f\"Success_{llm}\"], X_val_ngrams, X_test_ngrams\n",
    "        )\n",
    "\n",
    "        for predictive_method, kwargs, pred_method_name in predictive_method_list:\n",
    "\n",
    "            assessors_results_df = evaluate_and_update_arrays(\n",
    "                assessors_results_df,\n",
    "                X_train_ngrams_selected,\n",
    "                train_df[f\"Success_{llm}\"],\n",
    "                X_val_ngrams_selected,\n",
    "                validation_df[f\"Success_{llm}\"],\n",
    "                X_test_ngrams_selected,\n",
    "                test_df[f\"Success_{llm}\"],\n",
    "                predictive_method,\n",
    "                pred_method_name,\n",
    "                f\"ngrams_{n_gram_size}\",\n",
    "                llm,\n",
    "                filename,\n",
    "                **kwargs,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "lorenzo_predbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
